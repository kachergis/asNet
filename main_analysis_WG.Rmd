---
title: "WG"
author: "_"
output: html_document
---
# run from the beginning 
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(tidyr)
library(stringr)
library(boot)
library(purrr)
library(ggplot2)
library(ggthemes)
library(feather)
library(poweRlaw)
library(wordbankr)
library(rwebppl)
library(lme4)

#Load helper functions
source(paste(getwd(),"/helper_functions/all_helper.r",sep = ""), chdir = T)

#http://macappstore.org/espeak/

#Load probabilsitic models
source(paste(getwd(),"/models/all_models.r",sep = ""), chdir = T)

```


Load data_AS_WG
```{r}
data_AS_WG <- read.delim("mci_words_gestures01.txt", header = TRUE, sep = "\t", dec = ".")

# extracting first row as a descriptive dataframe
description_WG <- data_AS_WG[1:1,]
description_WG <- as.data.frame(t(description_WG))
names(description_WG) <- "description"

# we only kept vocab 
eliminated_WG <- data_AS_WG%>%
  select(c(23:58,454:520))

#using mci_words_gestures01_id	as a disctincetive id and The NDAR Global Unique Identifier 

data_raw_AS_WG <- data_AS_WG %>%
  select(c("mci_words_gestures01_id","subjectkey","interview_age", 59:453))

colnames(data_raw_AS_WG) <- as.character(unlist(data_raw_AS_WG[1,])) #unlist the row
data_raw_AS_WG = data_raw_AS_WG[-1, ]

# what are the duplicated
AS_WG_duplicated <- data_raw_AS_WG[duplicated(colnames(data_raw_AS_WG))] # can call colnames 

# making column names unique
names(data_raw_AS_WG) <- make.unique(names(data_raw_AS_WG), sep="_")


data_raw_AS_WG<- data_raw_AS_WG %>%
  rename(id = "mci_words_gestures01_id",
         GUID = "The NDAR Global Unique Identifier (GUID) for research subject", 
         age = "Age in months at the time of the interview/test/sampling/imaging.",
         house = "MacArthur Words and Gestures: Vocabulary Checklist: House")%>%
  mutate(age = as.numeric(as.character(age)))


data_clean_AS_WG <- data_raw_AS_WG %>%
  gather(key = "definition", value = "value", -c(id,GUID,age))%>%
  #separate(definition, c("category","definition"),sep = "\\. ") %>%
  mutate_all(na_if,"",)%>% #if blank then fill in NA
   mutate(value = ifelse(value == 0, FALSE, TRUE))

data_all_AS_WG <- data_clean_AS_WG %>%
     group_by(definition, age) %>%
       summarise(num_true = sum(value, na.rm = TRUE),
                 num_false = n() - num_true,
                 prop= num_true/n())

summary(data_all_AS_WG)
```

Compute Age of Acqusition (AoA) of definitions

#https://github.com/langcog/wordbank-book/blob/master/104-appendix-aoa.Rmd
baysian GLM

```{r}
fit_inst_measure_uni <- function(inst_measure_uni_data) {
  ages <- min(inst_measure_uni_data$age):max(inst_measure_uni_data$age)
  model <- glm(cbind(num_true, num_false) ~ age, family = "binomial",
               data = inst_measure_uni_data)
  fit <- predict(model, newdata = data.frame(age = ages), se.fit = TRUE)
  aoa <- -model$coefficients[["(Intercept)"]] / model$coefficients[["age"]]
  constants <- inst_measure_uni_data %>%
    ungroup()%>%
    select(definition) %>%
    distinct()
  
  props <- inst_measure_uni_data %>%
    ungroup() %>%
    select(age, prop)
  
  data.frame(age = ages,
             fit_prop = inv.logit(fit$fit),
             fit_se = fit$se.fit,
             aoa_AS = aoa, 
             definition = constants$definition) %>%
    left_join(props)
}


list_by_item <- data_all_AS_WG %>% 
  rename(num_true = num_true,
         num_false = num_false,
         prop = prop)%>%
  split(paste(.$definition)) # need comfirmation 

data_aoa_AS_WG <- map(list_by_item, fit_inst_measure_uni) %>% 
  bind_rows() %>%
  select(definition,aoa_AS) %>%
  distinct()

list_of_values <- c("chicken","fish_1","water_1","drink","swing_1","watch","clean_1","backyard_1")

data_aoa_AS_WG<- data_aoa_AS_WG %>%
  filter(!definition %in% list_of_values) # get rid of thoes rows

# dropping lower aoa 

#chicken	78.88241
#chicken_1	77.25093

#	fish	67.75401
#	fish_1	92.03162

# water	65.32196
#	water_1	68.53121

#	drink	65.18341
#	drink_1	62.98884

#	swing	65.53841
#	swing_1	68.41339

#	watch	114.3912
#	watch_1	104.8288

#	clean	83.78244
#	clean_1	86.33049

#	backyard	98.45454
#	backyard_1	131.03545

#feather::write_feather(data_aoa_AS_WG, "saved_data/data_aoa_AS_WG.feather")
data_aoa_AS_WG <- feather::read_feather("saved_data/data_aoa_AS_WG.feather") 
```


```{r}
data_aoa_TD <- feather::read_feather("saved_data/data_aoa_TD.feather")%>%
  filter(measure == "understands")

all_aoa_WG <- inner_join(data_aoa_TD,data_aoa_AS_WG, by = c("uni_lemma" = "definition"))%>%
  rename(aoa_TD = aoa,
         aoa_AS = aoa_AS)%>%
  filter(aoa_TD > 0)%>% # note that i've applied filter here 
  mutate(
         diff = aoa_AS - aoa_TD,
         diff_perc = (aoa_AS - aoa_TD)/aoa_TD*100)%>%
  arrange(desc(diff_perc)) 

most_diff_aoa <-  all_aoa_WG %>% head(10)
least_diff_aoa <- tail(all_aoa_WG, 10)

diff_aoa <- bind_rows(least_diff_aoa,most_diff_aoa)
```

Visualizations:

Difference in AoAs

```{r}

ggplot(diff_aoa, aes(x = lexical_class, y = diff_perc))+
  geom_point(alpha = 0.8)+
  labs(y = "Percentage Change in AoA", x = "Lexical Class", title = "Top & Bottom 10 in Percentage difference in AoA")+
  theme_bw() # the word daddy has a huge percent change in the current method of computing AoA
  

ggplot(all_aoa_WG, aes(aoa_TD))+
  stat_density(geom="line")+
  geom_density(aes(aoa_AS),color = "red")+
  theme_classic()+
  labs(x = "AoA", title = "Density plot of Age of Acqusition", caption = "Black: TD, Red = AS_method")#+
  #facet_wrap(vars(lexical_class)) # Much bigger gap compared with WS

```

Median Produced comparison 

```{r}
data_median_TD_WG <- feather::read_feather("saved_data/data_median_TD.feather") %>%
  filter(form == "WG")

data_median_WG <- data_clean_AS_WG %>%
  filter(value == TRUE)%>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition)) %>%
  group_by(age) %>%
  summarise(value = median(n))%>%
  filter(age <= 50)%>% # for zoom in the difference
  ggplot(aes(x = age, y = value))+
  #geom_point(col = "dodgerblue")+
  geom_smooth()+
  labs(y = "Median Gesture Understanding & Production", x = "Age(Month)",title = "Median Gesture Understanding & Production Comparison", caption = "Orange: TD, Blue: AS")+
  geom_smooth(data = data_median_TD_WG, aes(x = age, y = production), color = "darkorange")+
  theme_classic()

data_median_WG

```

Percentile graph 

```{r}
taus <-  c(0.1, 0.25, 0.5, 0.75, 0.9)

data_children <- data_clean_AS_WG %>%
  filter(value == TRUE, age <= 40) %>% # data looks too sparse after age 40
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition))%>%
  ggplot(aes(x  = age, y = n)) +
  geom_jitter(width = .4, size = 1, alpha = .6) +
  labs("Production (number of words)", title = "Gesture Understanding & Production vs. Age in AS children") +
  ylim(c(0, 400)) + 
  theme(legend.position = "bottom")+
  geom_quantile(quantiles = taus)

data_children

```


Data exploration on age groups of children with AS. Try to exclude age groups that have too few of data points 

```{r}
count <- data_raw_AS_WG %>%
  group_by(age)%>%
  summarise(N = n())

ggplot(count, aes(x = age, y = N))+
  geom_line()+
  labs(y = "Number of Children on Record", title = "Number of Children with Autism on Record")

count_a15 <- count %>% filter(N > 15) ## count above 15, 15 is simply a random number picked, could be futher adjusted. The range is 8 to 62

```

Replotting visualizations while excluding age categories that have fewer than 15 children in the group

```{r}
data_median_n <- left_join(count_a15, data_clean_AS_WG, by = "age") %>%
  filter(value == TRUE)%>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition)) %>%
  group_by(age) %>%
  summarise(value = median(n))

data_median_n %>%
  filter(age <= 62, age >= 8)%>%
  ggplot(aes(x = age, y = value))+
  #geom_point(col = "dodgerblue")+
  geom_smooth()+
  labs(y = "Median Gesture Understanding & Production", x = "Age(Month)",title = "Median Gesture Understanding & Production Comparison", caption = "Orange: TD, Blue: AS. Without data point that have less than 15 obersvations")+
  geom_smooth(data = data_median_TD_WG, aes(x = age, y = production), color = "darkorange")+
  theme_classic()


data_children_n <- left_join(count_a15, data_clean_AS_WG, by = "age")%>%
  filter(value == TRUE) %>%
  group_by(GUID,age)%>%
  summarise(n = n_distinct(definition))

ggplot(data_children_n,
        aes(x  = age, y = n)) +
  geom_jitter(width = .4, size = 1, alpha = .4) +
  labs("Production (number of words)", title = "Production vs. Age in AS children",caption = "Without data point that have less than 15 obersvations") +
  ylim(c(0, 400)) + 
  theme(legend.position = "bottom")+
  geom_quantile(quantiles = taus)

# we don't see anything during month 50, there fore the range is limited to 8 to 49

```

Two word comparison. The visualizations that excluded data points having less than 15 observations show clearer trend

```{r}
data_all_TD_WG <- feather::read_feather("saved_data/data_all_TD.feather") %>%
  filter(form == "WG")

ball_AS <- data_all_AS_WG%>%
  filter(definition == "ball", age <= 49)

ball_AS_clean <- left_join(count_a15, ball_AS, by = "age")

ball_TD <- data_all_TD_WG %>%
  filter(definition == "ball",
         form == "WG")

mod_1<- glm(prop ~ age, family = "binomial",
               data = ball_TD)

fitted_points_ball_TD <- mod_1 %>%
  broom::augment()%>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))

ggplot(ball_AS, aes(x = age, y = prop))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_point(data = ball_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could under stand & produce the gesture: Ball") +
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()


play_AS <- data_all_AS_WG %>%
  filter(definition == "play",
         age <= 49)

play_AS_clean <- left_join(count_a15, play_AS, by = "age")

play_TD <- data_all_TD_WG %>%
  filter(definition == "play")%>%
  filter(form == "WG")

mod_2<- glm(prop ~ age, family = "binomial",
               data = play_TD)

fitted_points_play_TD <- mod_2 %>%
  broom::augment()%>%
  mutate(fitted_prob = 1/(1 + exp(-.fitted)))

ggplot(play_AS, aes(x = age, y = prop))+
  geom_point()+
  geom_smooth(se = FALSE,method = 'loess')+
  geom_smooth(data = fitted_points_play_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = play_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could understand & produce the gesture: play") +
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks =c(0,10,20,30,40,50))+
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme_classic()

ggplot(ball_AS_clean, aes(x = age, y = prop))+
  geom_point()+
  geom_smooth(method = 'loess')+
    geom_smooth(data = fitted_points_ball_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = ball_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could understand & produce the guesture of: Ball", caption = "Without data point that have less than 15 obersvations") +
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme(legend.position = "bottom")+
  theme_classic()

ggplot(play_AS_clean, aes(x = age, y = prop))+
  geom_point()+
  geom_smooth(methosd = 'loess')+
  geom_smooth(data = fitted_points_play_TD, aes(x =age, y = fitted_prob), col = "red")+
  geom_point(data = play_TD, aes(x =  age, y = prop), col = "red")+
  labs(y = "Prop", x = "Age (Month)", title = "Proportion of Children who could understand & produce the guesture of: play", caption = "Without data point that have less than 15 obersvations") +
  theme(legend.position = "bottom")+
  scale_x_continuous(breaks =c(0,10,20,30,40,50))+
  geom_hline(yintercept=.5, linetype="dashed", color = "blue")+
  theme_classic()

```

Make data growth dataframe

```{r}
make_data_growth <- function(word_aoa) {
  
  measure <- unique(word_aoa$measure)
  lang <- unique(word_aoa$language)
    
  ages<- (word_aoa %>%
    arrange(age) %>%
    group_by(age) %>%
    summarise(n = n()))$age
  
  df <- data.frame()
  for (i in ages) {
    rem_words <- word_aoa %>% filter(age >= i)
    rem_lemma <- c(rem_words$uni_lemma)
    rem_def <- c(rem_words$definition)
    rem_item<- c(rem_words$item)
    corr_age <- rep(i, times = length(rem_lemma))
    curr_df <- data.frame(corr_age, rem_item, rem_lemma, rem_def)
    df <- rbind(df, curr_df)
  }  
  df <- df %>% rename(uni_lemma = rem_lemma, definition=rem_def, item=rem_item)%>%
    left_join(word_aoa %>% select(item, age)) %>%
    mutate(learned = as.numeric(age == corr_age)) %>%
    select(corr_age, item, definition, uni_lemma,learned) %>%
    rename(age = corr_age) %>%
    arrange(age, item) %>%
    mutate(language = lang,
           measure = measure)
  return(df)
}

aoa_by_measure_lang <- all_aoa_WG %>%
  mutate(age = round(aoa_AS)) %>%
  select(-form, -aoa_AS) %>%
  filter(lexical_class == "nouns")%>%
  split(paste(.$language, .$measure))
  
  
data_growth_AS_WG <- map(aoa_by_measure_lang, make_data_growth) %>%
  bind_rows() # age range from 56 month to 248 month?

#feather::write_feather(data_growth_AS_WG,"saved_data/data_growth_AS_WG.feather")

data_growth_AS_WG <- feather::read_feather("saved_data/data_growth_AS_WG.feather") 
```


Construct the netwroks and derive the dynamic predictors: PAC and PAT
```{r}
words_growth <- data_growth_AS_WG

growth_by_meas <- words_growth %>%
  split(paste(.$measure))

growth_by_meas_lang <- words_growth %>%
  split(paste(.$measure, .$language))

#Semantic netwrok 
##################

sem_net_fun  <- function (growth_meas_lang){
  
  first_age<- growth_meas_lang$age[1]
  
  lemma_list<- growth_meas_lang %>%
      trim_all_unilemma() %>% #For semantic netowtks only the unilmma
      filter(age==first_age) %>%
      select(item, uni_lemma)

assoc_pairs<- make_assoc_pairs(lemma_list = lemma_list)

assoc_PAC<- PAC_generator(vocab_age = growth_meas_lang, word_pairs = assoc_pairs) %>% 
      rename(PAC_assoc = value) %>% select(-definition, -uni_lemma)

 assoc_PAT<- PAT_generator(vocab_age = growth_meas_lang, word_pairs = assoc_pairs) %>% 
      rename(PAT_assoc = value) %>% select(-definition, -uni_lemma)
 
 sem_growth <- growth_meas_lang %>%
   left_join(assoc_PAC) %>%
   left_join(assoc_PAT)
 
 return(sem_growth)
 
}

sem_red <-  map(growth_by_meas_lang, sem_net_fun) %>%
  bind_rows()

#feather::write_feather(sem_red, "saved_data/sem_red_AS_WG.feather")
sem_red <- feather::read_feather("saved_data/sem_red_AS_WG.feather") 

#Phonological Networks
######################


phono_net_fun <- function (growth_meas_lang) {
  
  lang <- unique(growth_meas_lang$language)
  #meas <- unique(growth_meas_lang$measure)
  
  first_age<- growth_meas_lang$age[1]
  
  def_list<- growth_meas_lang %>%
      trim_all_definition() %>% 
      filter(age==first_age) %>%
      select(item, definition)
  
  phono_pairs<- make_IPA_pairs(def_list = def_list, lang = lang)
  
  #Threshold the phonetic distance (we take t=2, becuase t=1 is too sparse) 
  threshold <- 2

  phono_PAC <- PAC_generator(vocab_age = growth_meas_lang, word_pairs = phono_pairs %>% IPA_threshold(threshold)) %>% 
      rename(PAC_phono_t2 = value) %>% select(-definition, -uni_lemma)
  
  phono_PAT <- PAT_generator(vocab_age = growth_meas_lang, word_pairs = phono_pairs %>% IPA_threshold(threshold)) %>% 
      rename(PAT_phono_t2 = value) %>% select(-definition, -uni_lemma)
  
  phono_growth <- growth_meas_lang %>%
   left_join(phono_PAC) %>%
   left_join(phono_PAT)
  
}

phono_red <-  map(growth_by_meas_lang, phono_net_fun) %>%
  bind_rows() 


#feather::write_feather(phono_red, "saved_data/phono_red_AS_WG.feather")
phono_red <- feather::read_feather("saved_data/phono_red_AS_WG.feather") 

#Combine data 
data_growth_net <- words_growth %>%
  left_join(sem_red) %>%
  left_join(phono_red)

#feather::write_feather(data_growth_net, "saved_data/data_growth_net_AS_WG.feather")
```

```{r,fig.width=10, fig.height=5}
#Compute other static predictors (frequency and length) besides PAC

data_static <- data_growth_net %>%
  distinct(language,uni_lemma, definition, item, PAC_assoc, PAC_phono_t2) %>%
  rename(sem_deg = PAC_assoc, 
         phono_deg =  PAC_phono_t2)
  
#Word length
words_len <- data_static %>%
  trim_all_definition() %>%
  rowwise() %>%
  mutate(IPA=Speak(lang = language, word = definition)) %>%
  trim_IPA_completely() %>%
  mutate(length = str_count(IPA)) %>%
  select(-definition, -uni_lemma, -sem_deg, -phono_deg)

load("saved_data/uni_joined.RData")
frequency_mika <- uni_joined %>%
  filter(lexical_classes =='nouns') %>%
  distinct(language, uni_lemma, frequency) %>%
  mutate(lang_temp = ifelse(language == "French (Quebec)", "French (Quebecois)", language)) %>%
  select(-language) %>%
  rename(language = lang_temp)

words_freq <- data_static %>%
  left_join(frequency_mika) %>%
  select(-definition, -uni_lemma, -sem_deg, -phono_deg)

aoa_items <- all_aoa_WG %>%
  select(language, measure, item, aoa_AS) # Here I should round the AoA?
  
#Combine predictors
data_static_pred_AS_WG  <- data_static %>%
  left_join(words_len) %>%
  left_join(words_freq) %>%
  left_join(aoa_items) #%>%
  #select(-definition, -uni_lemma)

#feather::write_feather(data_static_pred_AS_WG, "saved_data/data_static_pred_AS_WG.feather")
data_static_pred_AS_WG <- feather::read_feather("saved_data/data_static_pred_AS_WG.feather") 

#Combine with full proportion-based data  (for the second regression which fit the entire production curve)
data_static_pred_AS_WG$language <- plyr::mapvalues(data_static_pred_AS_WG$language, 
                                 from = "English (American)", 
                                 to = "English")


#data_static_prop$language <- plyr::mapvalues(data_static_prop$language, 
#                                 from = "English (American)", 
#                                 to = "English")

#data_static_prop <- data_all %>%
#  left_join(data_static_pred_AS_WG) 

```

```{r}
Production

unilemmas <- data_static_pred_AS_WG %>%
  select(-IPA) %>%
  rename(aoa = aoa_AS)%>%
  filter(!is.na(uni_lemma)) %>% 
  filter(!is.na(sem_deg))  #only keep unilemmas that intersect with free association data

#problems need to check 
uni_scale <- unilemmas %>%
  group_by(measure, language) %>%
  mutate_at(c('sem_deg', 'phono_deg', 'length', 'frequency'), funs(as.numeric(Hmisc::impute(.)))) %>%
  mutate_at(c('sem_deg', 'phono_deg', 'length', 'frequency'), funs(as.numeric(scale(.))))

#feather::write_feather(uni_scale, "saved_data/uni_scale_AS_WG.feather")
uni_scale <- feather::read_feather("saved_data/uni_scale_AS_WG.feather")

  gather(predictor, value, sem_deg:frequency) %>%
  filter(predictor == "sem_deg" | predictor == "phono_deg") 

correlations <- data_long %>%
  group_by(measure, language, predictor) %>%
  summarise(cor = round(cor(aoa, value), 2))

plot_correlation_prod <- ggplot(data_long, aes(x=value, y=aoa))+
  facet_grid(predictor ~ language)+
  geom_jitter(size = 0.9,col = "lightblue")+
  geom_abline(slope = -1)+
  coord_cartesian(xlim=c(-1,5))+
  #scale_x_continuous(limits=c(-2,5))+
  scale_y_continuous(breaks =c(55,85,115,145))+
  geom_smooth(method = "lm", colour = "grey1", se=FALSE)+
  #scale_colour_solarized(name = "") +
  theme_few()+
  theme(aspect.ratio = 0.7, 
        plot.margin=grid::unit(c(0,0,0,0), "mm")
        )+
  geom_text(data=subset(correlations), aes(label=paste("r=", cor, sep="")), x=3.5, y=100, size=4, fontface = "bold")+
  xlab("degree z-score") +ylab("AoA")
  
plot_correlation_prod
```


### Degree distribution
Import the analysese from cogsci paper

```{r}

#Data for plot
degreeDist <- data.frame(matrix(ncol = 5, nrow = 0))
dist_names <- c("x", "y", "dimension", "language", "measure")
colnames(degreeDist) <- dist_names
#Parameters and test
degreeTest <- data.frame(matrix(ncol = 6, nrow = 0))
test_names <- c("xMin", "alpha", "pVal", "dimension","language", "measure")
colnames(degreeTest) <- test_names
powerLaw_fun <- function(data_growth_meas_lang, analysis){
  
  lang <- unique(data_growth_meas_lang$language)
  meas <- unique(data_growth_meas_lang$measure)
  
  data_meas_lang <- data_growth_meas_lang %>%
    select(measure, language, sem_deg, phono_deg) %>%
    dplyr::rename(Sem = sem_deg, Phono = phono_deg) %>%
    dplyr::filter (!is.na(Sem),!is.na(Phono)) 
  
  ##Semantic network
  
  #fit and derive parameters for power law
  semList <- data_meas_lang$Sem[data_meas_lang$Sem != 0]
  sem_pl = displ$new(semList)
  sem_est = estimate_xmin(sem_pl)
  sem_pl$setXmin(sem_est)
  sem_dist = plot(sem_pl) %>%
    mutate(dimension='Sem', language  =  lang, measure = meas)
  
  #bootstrap to get p-value
  sem_boot = bootstrap_p(sem_pl, no_of_sims=1000, threads=2)
  
  sem_test <- data.frame(as.numeric(sem_pl$xmin), as.numeric(sem_pl$pars), as.numeric(sem_boot$p), 'Sem', lang, meas)
  colnames(sem_test) <- test_names
  
  ##Phonological network
  
  #fit and derive parameters for power law
  phonoList <- data_meas_lang$Phono[data_meas_lang$Phono != 0]
  phono_pl = displ$new(phonoList)
  phono_est = estimate_xmin(phono_pl)
  phono_pl$setXmin(phono_est)
  phono_dist = plot(phono_pl) %>%
    mutate(dimension='Phono', language = lang, measure = meas)
  
  #bootstrap to get p-value
  phono_boot = bootstrap_p(phono_pl, no_of_sims=1000, threads=2)
  
  phono_test <- data.frame(as.numeric(phono_pl$xmin), as.numeric(phono_pl$pars), as.numeric(phono_boot$p), 'Phono', lang, meas)
  colnames(phono_test) <- test_names
  
  dist_meas_lang <- bind_rows(sem_dist, phono_dist)
  test_meas_lang <- bind_rows(sem_test, phono_test)
  
  #return(dist_meas_lang)
  
  if (analysis == 'distribution') {
    
    return(dist_meas_lang)
    
  } else if (analysis == 'test') {
    
    return(test_meas_lang)
    
  } else {
    
    print("Please specify the analysis type ('distribution' or 'test')")
    
  }
  
}



#Split by measure and language


data_by_meas_lang <- unilemmas %>%
  split(paste(.$measure, .$language))

degree_dist  <- map2(data_by_meas_lang, 'distribution', powerLaw_fun) %>%
  bind_rows()


degree_test  <- map2(data_by_meas_lang, 'test' , powerLaw_fun) %>%
  bind_rows()

#feather::write_feather(degree_dist, "saved_data/degree_dist_AS_WG.feather")
degree_dist <- feather::read_feather("saved_data/degree_dist_AS_WG.feather")


#feather::write_feather(degree_test, "saved_data/degree_test_AS_WG.feather")
degree_test <- feather::read_feather("saved_data/degree_test_AS_WG.feather")

#plot cumulative distributions
ggplot(data = degree_dist, aes(x=x, y=y, col=dimension))+
  facet_grid(measure ~ language)+
  geom_point() +
  scale_y_log10() + scale_x_log10() +
  labs(x = "degree", y = "probability")+
  theme(aspect.ratio = 1)

```

 
## Growth mechanisms, see WS.rmd.


```{r}

sem_formula <- as.formula("aoa ~ sem_deg")
phono_formula <- as.formula("aoa ~ phono_deg")
all_formula <- as.formula("aoa ~ frequency + length + sem_deg + phono_deg")

formulas <- list(sem_formula,  phono_formula, all_formula)

coefs_all <- data.frame()


for (formula in formulas){
  
#Model
model_reg <- function(data) {
   lm(formula, data = data)
}

#CI
ci_reg <- function(data) {
   confint(lm(formula, data = data))
 }
  
reg <- uni_scale %>%
  group_by(measure, language) %>%
  nest() %>%
  mutate(model = map(data, model_reg)) %>%
  mutate(conf = map(data, ci_reg))

coefs_form <- reg %>%
  mutate(coefs = map(model, broom::tidy)) %>%
  mutate(ci = map(conf, broom::tidy)) %>%
  select(measure, language, coefs, ci) %>%
  unnest() %>%
  select(-.rownames) %>%
  dplyr::filter(term != "(Intercept)") %>%
  rename(predictor = term) %>%
  mutate(Test = ifelse(toString(formula[3])=='frequency + length + sem_deg + phono_deg', 'Combined', 'Individual'))

 coefs_all <- bind_rows(coefs_all, coefs_form)

}

feather::write_feather(coefs_all, "saved_data/static_preds_AS_WG.feather")
#coefs_all <- feather::read_feather("saved_data/static_preds_AS_WG.feather")

```

Plot for comprehension

```{r}
plot_reg_comp <- ggplot(subset(coefs_all, measure =="understands"), aes(x = predictor, y = estimate)) +
  geom_pointrange(aes(ymin = X2.5.., ymax = X97.5.., y = estimate, col = predictor, linetype=Test),
                  position = position_dodge(width = .5),
                  size = 0.5,
                  fatten = 0.5)+
                  
  geom_hline(yintercept = 0, color = "grey", linetype = "dashed")+
  #facet_wrap(~language, ncol=4)  +
  coord_flip() +
  guides(colour=FALSE)+
  scale_colour_solarized() +
  theme_few()+
  theme(aspect.ratio = 0.7)
 
plot_reg_comp
```
